{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (20, 14)\n",
    "import os\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from distributed import Client\n",
    "from earthio import load_array, LayerSpec\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "from dask_glm.datasets import make_regression\n",
    "from earthio.landsat_util import landsat_metadata\n",
    "from earthio.s3_landsat_util import SceneDownloader\n",
    "from elm.mldataset.cv_cache import CVCacheSampleId, cv_split\n",
    "from elm.model_selection import EaSearchCV\n",
    "from elm.model_selection.ea_searchcv import EaSearchCV\n",
    "from elm.pipeline import steps\n",
    "from elm.pipeline.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVR\n",
    "from xarray_filters import MLDataset\n",
    "from xarray_filters.datasets import _make_base\n",
    "from xarray_filters.pipeline import Generic, Step\n",
    "from xarray_filters.pipe_utils import data_vars_func\n",
    "from xarray_filters.pipe_utils import for_each_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LAYER_SPECS = [LayerSpec(search_key='name',  # should be guessed automatically that \"name\" is where to look - \n",
    "                                             # TODO name -> \"key\" instead of \"search_key\n",
    "                         search_value='B{}.TIF'.format(layer),  # this should have a shorter name like \"search\"\n",
    "                         name='layer_{}'.format(layer),         # TODO this should be autoguessed \"layer_0\" \"layer_1\",etc \n",
    "                         buf_xsize=800,                         \n",
    "                         buf_ysize=800) for layer in range(1, 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROW_PATH_MONTHS = dict(row=33, path=15, months=tuple(range(1,13)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NORMALIZED_DIFFS = ('nbr', 'ndsi', 'ndwi', 'ndvi')\n",
    "DEFAULT_LAYERS = [layer_spec.name for layer_spec in LAYER_SPECS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplerBase(Step):\n",
    "    func = None\n",
    "    def transform(self, kwargs, y=None, **kw):\n",
    "        params = self.get_params(deep=True).copy()\n",
    "        func = params.pop('func')\n",
    "        func = func or load_array\n",
    "        kw.update(params)\n",
    "        return func(**kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def s3_landsat_sample(**params):\n",
    "    params = params.copy()\n",
    "    s3_landsat = SceneDownloader()\n",
    "    layer_specs = params.pop('layer_specs')\n",
    "    clear_image = s3_landsat.lowest_cloud_cover_image(**params)\n",
    "    download_url = clear_image.download_url.values[0]\n",
    "    \n",
    "    local_files = s3_landsat.download_all_layers(download_url)\n",
    "    this_sample_dir = os.path.dirname(local_files[0])\n",
    "    X = load_array(this_sample_dir, layer_specs=LAYER_SPECS)\n",
    "    meta_files = [f for f in local_files if f.endswith('.txt')][0]\n",
    "    X.attrs.update(vars(landsat_metadata(meta_files)))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sampler(SamplerBase):\n",
    "    layer_specs = None\n",
    "    #func = s3_landsat_sample               # TODO it should work to put a function here but it doesn't\n",
    "    func = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@for_each_array\n",
    "def set_nans(arr):\n",
    "    arr = arr.copy(deep=True)\n",
    "    arr.values = arr.values.astype(np.float32)\n",
    "    arr.values[arr.values <= 1] = np.NaN\n",
    "    arr.values[arr.values == 2**16] = np.NaN\n",
    "    return arr\n",
    "\n",
    "class ForEachStep(Step):\n",
    "    keep_attrs = True\n",
    "    func = None     \n",
    "    pass_attrs = False\n",
    "    def transform(self, X, y=None, **kw):\n",
    "        kw = kw.copy()\n",
    "        kw.update(self.get_params(deep=True).copy())\n",
    "        # TODO here should we filter args and kwargs (see func_signatures.py in xarray_filters)\n",
    "        if kw.pop('pass_attrs'):\n",
    "            kw['attrs'] = X.attrs\n",
    "        dset = kw.pop('func')(X, **kw)\n",
    "        if kw.pop('keep_attrs', True):\n",
    "            dset.attrs.update(X.attrs)\n",
    "        return dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normed_diff(a, b):\n",
    "    return (a - b) / (a + b)\n",
    "\n",
    "@data_vars_func\n",
    "def normalized_diffs(**dset):\n",
    "    print('Called with ', dset.keys())\n",
    "    dset['ndwi'] = normed_diff(dset['layer_4'], dset['layer_5'])\n",
    "    dset['ndvi'] = normed_diff(dset['layer_5'], dset['layer_4'])\n",
    "    dset['ndsi'] = normed_diff(dset['layer_2'], dset['layer_6'])\n",
    "    dset['nbr']  = normed_diff(dset['layer_4'], dset['layer_7'])\n",
    "    return dset\n",
    "\n",
    "class DataVarsStep(Step):                             # TODO - add this to xarray_filters and test\n",
    "    func = None                                       # func should have signature of **data_vars\n",
    "                                                      # (expecting data_vars of \"X\")\n",
    "    def transform(self, X, y=None, **kw):\n",
    "        print('kw1', kw, self.get_params(deep=True))\n",
    "        kw = kw.copy()\n",
    "        kw.update(self.get_params(deep=True).copy())\n",
    "        print('kw', kw)\n",
    "        # TODO here should we filter args and kwargs (see func_signatures.py in xarray_filters)\n",
    "        return kw.pop('func')(dset=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@for_each_array\n",
    "def to_radiance_or_reflectance(arr, attrs=None, to='REFLECTANCE', **kw):\n",
    "    num = arr.name.split('_')[-1]\n",
    "    add = attrs.get('{}_ADD_BAND_{}'.format(to, num))\n",
    "    mult = attrs.get('{}_MULT_BAND_{}'.format(to, num))\n",
    "    arr.values[:] = arr.values * mult + add\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_bands(X, layers=None, include_normed_diffs=True, **kw):\n",
    "    new = OrderedDict()\n",
    "    for layer in layers:\n",
    "        data_arr = getattr(X, layer)\n",
    "        new[layer] = data_arr\n",
    "    if include_normed_diffs:\n",
    "        for diff in NORMALIZED_DIFFS:\n",
    "            new[diff] = getattr(X, diff)\n",
    "    return MLDataset(new)\n",
    "\n",
    "'''\n",
    "class ChooseBands(Generic):                   # TODO - this section should work but currently doesn't\n",
    "    include_normed_diffs = True\n",
    "    layers = DEFAULT_LAYERS\n",
    "    func = choose_bands\n",
    "'''\n",
    "class ChooseBands(Generic):\n",
    "    include_normed_diffs = True\n",
    "    layers = None\n",
    "    func = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "choose_step = ChooseBands(func=choose_bands, layers=DEFAULT_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DropRows(Step):                                    # TODO - this could be a built in step in xarray_filters\n",
    "    def transform(self, X, y=None, **kw):                #e.g from xarray_filters.steps import DropRows;d=DropRows();d.fit_transform(X)\n",
    "        X = X.to_features()\n",
    "        features = X.features.dropna('space', how='any')\n",
    "        return MLDataset(OrderedDict([('features', features)]))\n",
    "    fit = transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the changes above, some of the constructors could be simplified, e.g. not passing \"func=something\"\n",
    "dset0 = Sampler(func=s3_landsat_sample, layer_specs=LAYER_SPECS).fit_transform(ROW_PATH_MONTHS)\n",
    "dset1 = ForEachStep(func=set_nans).fit_transform(dset0)\n",
    "dset2 = ForEachStep(func=to_radiance_or_reflectance, pass_attrs=True).fit_transform(dset1)\n",
    "dset3 = DataVarsStep(func=normalized_diffs).fit_transform(dset2)\n",
    "dset4 = DropRows().fit_transform(dset3)\n",
    "# These steps return numpy arrs\n",
    "np_arr0 = steps.preprocessing.StandardScaler().fit_transform(dset4)\n",
    "np_arr1 = steps.decomposition.PCA(n_components=5).fit_transform(np_arr0)\n",
    "est = steps.cluster.MiniBatchKMeans()\n",
    "fitted = est.fit(np_arr1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = est.predict(np_arr1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {'est__n_clusters': list(range(8, 12)),\n",
    "                       'choose__include_normed_diffs': [True, False],\n",
    "                       'pca__n_components': list(range(5, 12))}\n",
    "\n",
    "pipe = Pipeline([('sampler', Sampler(func=s3_landsat_sample)),\n",
    "                 ('set_nans', ForEachStep(func=set_nans)),\n",
    "                 ('radiance', ForEachStep(func=to_radiance_or_reflectance, pass_attrs=True)),\n",
    "                 ('normed_diffs', DataVarsStep(func=normalized_diffs)),\n",
    "                 ('drop_na', DropRows()),\n",
    "                 ('standard', steps.preprocessing.StandardScaler()),\n",
    "                 ('pca', steps.decomposition.PCA(n_components=5)),\n",
    "                 ('est', steps.cluster.MiniBatchKMeans())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_selection = {\n",
    "    'select_method': 'selNSGA2',\n",
    "    'crossover_method': 'cxTwoPoint',\n",
    "    'mutate_method': 'mutUniformInt',\n",
    "    'init_pop': 'random',\n",
    "    'indpb': 0.5,\n",
    "    'mutpb': 0.9,\n",
    "    'cxpb':  0.3,\n",
    "    'eta':   20,\n",
    "    'ngen':  2,\n",
    "    'mu':    16,\n",
    "    'k':     8, # TODO ensure that k is not ignored - see elm issue #218\n",
    "    'early_stop': None,\n",
    "}\n",
    "\n",
    "fitted = pipe.fit(ROW_PATH_MONTHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following fails due to https://github.com/ContinuumIO/elm/issues/215\n",
    "'''\n",
    "ea = EaSearchCV(pipe,\n",
    "                param_distributions=param_distributions,\n",
    "                ngen=2,\n",
    "                model_selection=model_selection,\n",
    "                cv=3)\n",
    "Xt, y = ea.fit(ROW_PATH_MONTHS)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
