DASK_EXECUTOR: SERIAL
DASK_PROCESSES: null
DASK_SCHEDULER: null
DASK_THREADS: null
LADSWEB_LOCAL_CACHE: null
data_sources:
  S3_LANDSAT_L2_TIFS:
    band_specs:
    - {name: band_1, search_key: name, search_value: _B1.TIF}
    - {name: band_2, search_key: name, search_value: _B2.TIF}
    - {name: band_3, search_key: name, search_value: _B3.TIF}
    - {name: band_4, search_key: name, search_value: _B4.TIF}
    batch_size: 1440000
    get_y_func: elm.pipeline.tests.util:example_get_y_func_continuous
    reader: tif_example
    sample_args_generator: tif_file_gen
    sample_from_args_func: elm.sample_util.samplers:image_selection
    selection_kwargs: {file_pattern: TIF, top_dir: 'env:ELM_EXAMPLE_DATA_PATH'}
ensembles:
  no_ensemble: {init_ensemble_size: 1, ngen: 1, partial_fit_batches: 1, saved_ensemble_size: 1}
  small_ensemble: {init_ensemble_size: 16, ngen: 4, partial_fit_batches: 4, saved_ensemble_size: 4}
feature_selection:
  select_all:
    choices: all
    kwargs: {}
    scoring: chi2
    scoring_kwargs: {}
    selection: all
  top_80_percent:
    choices: all
    kwargs: {percentile: 80}
    scoring: f_classif
    selection: sklearn.feature_selection:SelectPercentile
model_scoring:
  accuracy_score_cv: {greater_is_better: true, needs_proba: false, needs_threshold: false,
    scoring: accuracy_score, scoring_agg: 'numpy:median'}
  kmeans_aic:
    score_weights: [-1]
    scoring: elm.model_selection.kmeans:kmeans_aic
model_selection:
  kmeans_model_averaging:
    func: elm.model_selection.kmeans:kmeans_model_averaging
    kwargs: {drop_n: 0, evolve_n: 1, init_n: 0}
  select_top_n:
    func: elm.model_selection.base:select_top_n_models
    kwargs: {top_n: 1}
pipeline:
- data_source: S3_LANDSAT_L2_TIFS
  sample_pipeline:
  - {sample_pipeline: flatten_example}
  - {flatten: C}
  - {random_sample: 10000}
  - {get_y: true}
  steps:
  - {method: fit, train: kmeans}
  - {predict: kmeans}
predict:
  kmeans: {}
readers:
  hdf4_example: {load_array: 'elm.readers.hdf4:load_hdf4_array', load_meta: 'elm.readers.hdf4:load_hdf4_meta'}
  hdf5_example: {load_array: 'elm.readers.hdf5:load_hdf5_array', load_meta: 'elm.readers.hdf5:load_hdf5_meta'}
  netcdf_example: {load_array: 'elm.readers.netcdf:load_netcdf_array', load_meta: 'elm.readers.netcdf:load_netcdf_meta'}
  tif_example: {load_array: 'elm.readers.tif:load_dir_of_tifs_array', load_meta: 'elm.readers.tif:load_dir_of_tifs_meta'}
sample_args_generators: {iter_files_recursively: 'elm.readers.local_file_iterators:iter_files_recursively',
  tif_file_gen: 'elm.readers.local_file_iterators:iter_dirs_of_dirs'}
sample_pipelines:
  flatten_example:
  - {select_canvas: band_1}
  - {flatten: C}
  - {sklearn_preprocessing: require_positive}
  - {drop_na_rows: true}
  log10:
  - {sklearn_preprocessing: require_positive}
  - {sklearn_preprocessing: log10}
  standardize_log10:
  - {sample_pipeline: log10}
  - {sklearn_preprocessing: standard}
  standardize_log10_var_top_80:
  - {sample_pipeline: standardize_log10}
  - {feature_selection: top_80_percent}
  standardize_log10_var_top_80_inter:
  - {sample_pipeline: standardize_log10_var_top_80}
  - {sklearn_preprocessing: poly_2nd_interactions_only}
  standardize_log10_var_top_80_inter_trans:
  - {sample_pipeline: standardize_log10_var_top_80_inter}
  - {method: fit_transform, transform: pca}
  standardize_log10_var_top_80_trans:
  - {sample_pipeline: standardize_log10_var_top_80}
  - {method: fit_transform, transform: pca}
sklearn_preprocessing:
  log: {func: 'numpy:log', method: FunctionTransformer, validate: true}
  log10: {func: 'numpy:log10', method: FunctionTransformer, validate: true}
  min_max:
    axis: 0
    copy: false
    feature_range: [0, 1]
    method: minmax_scale
  poly_2nd: {degree: 2, include_bias: true, interaction_only: false, method: PolynomialFeatures}
  poly_2nd_interactions_only: {degree: 2, include_bias: true, interaction_only: true,
    method: PolynomialFeatures}
  require_positive:
    func: elm.sample_util.encoding_scaling:require_positive
    func_kwargs: {small_num: 0.0001}
    method: FunctionTransformer
  standard: {copy: false, method: StandardScaler, with_mean: true, with_std: true}
train:
  kmeans:
    ensemble: no_ensemble
    ensemble_kwargs: {init_ensemble_size: 2, ngen: 1, partial_fit_batches: 1, saved_ensemble_size: 1}
    fit_method: fit
    model_init_class: sklearn.linear_model:ElasticNet
    model_init_kwargs: {max_iter: 10}
    model_scoring: null
    model_selection: null
    output_tag: kmeans
transform:
  pca:
    ensemble: no_ensemble
    model_init_class: sklearn.decomposition:IncrementalPCA
    model_init_kwargs: {n_components: 2}
    model_scoring: null
