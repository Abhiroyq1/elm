readers: {
  hdf4_example: {
    load_array: "elm.readers.hdf4:load_hdf4_array",
    load_meta: "elm.readers.hdf4:load_hdf4_meta"
  },
  tif_example: {
    load_array: "elm.readers.tif:load_dir_of_tifs_array",
    load_meta: "elm.readers.tif:load_dir_of_tifs_meta",
  },
  hdf5_example: {
    load_array: "elm.readers.hdf5:load_hdf5_array",
    load_meta: "elm.readers.hdf5:load_hdf5_meta",
  },
  netcdf_example: {
    load_array: "elm.readers.netcdf:load_netcdf_array",
    load_meta: "elm.readers.netcdf:load_netcdf_meta",
  }
}

data_sources: {
 hdf5_precip_hourly: {
   reader: hdf5_example,
   sample_from_args_func: "elm.sample_util.samplers:image_selection",
   band_specs: [{search_key: sub_dataset_name,
                 search_value: /precipitation,
                 name: band_1,
                 key_re_flags: [],
                 value_re_flags: [],
                 meta_to_geotransform: "elm.readers.util:grid_header_to_geo_transform",
                 stored_coords_order: ["x", "y"]}],
   file_pattern: "3B-MO.MS.MRG.3IMERG.20160101-S000000-E235959.01.V03D.HDF5",
   top_dir: "env:ELM_EXAMPLE_DATA_PATH",
   batch_size: 1440000,
   sample_args_generator: "elm.readers.local_file_iterators:iter_files_recursively",
  },
 NPP_DSRF1KD_L2GD: {
  reader: hdf4_example,
  sample_from_args_func: "elm.sample_util.samplers:image_selection",
  band_specs: [{search_key: long_name, search_value: "Band 1 ", name: band_1},
  {search_key: long_name, search_value: "Band 2 ", name: band_2},
  {search_key: long_name, search_value: "Band 3 ", name: band_3},
  {search_key: long_name, search_value: "Band 4 ", name: band_4},
  {search_key: long_name, search_value: "Band 5 ", name: band_5},
  {search_key: long_name, search_value: "Band 6 ", name: band_6},
  {search_key: long_name, search_value: "Band 7 ", name: band_7},
  {search_key: long_name, search_value: "Band 9 ", name: band_9},
  {search_key: long_name, search_value: "Band 10 ", name: band_10},
  {search_key: long_name, search_value: "Band 11 ", name: band_11}],
  sample_args_generator: iter_files_recursively,
  top_dir: "env:ELM_EXAMPLE_DATA_PATH",
  metadata_filter: "elm.sample_util.metadata_selection:example_meta_is_day",
  file_pattern: "\\.hdf",
  batch_size: 1440000,
 },
 S3_LANDSAT_L2_TIFS: {
  sample_args_generator: tif_file_gen,
  top_dir: "env:ELM_EXAMPLE_DATA_PATH",
  file_pattern: ".TIF",
  reader: tif_example,
  sample_from_args_func: "elm.sample_util.samplers:image_selection",
  band_specs: [
  {search_key: name, search_value: "_B1.TIF", name: band_1},
  {search_key: name, search_value: "_B2.TIF", name: band_2},
  {search_key: name, search_value: "_B3.TIF", name: band_3},
  {search_key: name, search_value: "_B4.TIF", name: band_4},
  ],
  batch_size: 1440000,
  },
}
sample_args_generators: {
  tif_file_gen: "elm.readers.local_file_iterators:iter_dirs_of_dirs",
  iter_files_recursively: "elm.readers.local_file_iterators:iter_files_recursively",
}

feature_selection: {
  select_all: {
    selection: all,
    scoring: chi2,
    scoring_kwargs: {},
    kwargs: {},
    choices: all,
  },
  top_80_percent: {
    selection: "sklearn.feature_selection:SelectPercentile",
    kwargs: {percentile: 80},
    scoring: f_classif,
    choices: all,
  }
}
model_scoring: {
  accuracy_score_cv: {
    scoring: "accuracy_score",
    scoring_agg: "numpy:median",
    greater_is_better: True,
    needs_proba: False,
    needs_threshold: False,
  },
  kmeans_aic: {
    scoring: "elm.model_selection.kmeans:kmeans_aic",
    score_weights: [-1],
  }
}
model_selection: {
  kmeans_model_averaging: {
    kwargs: {
      drop_n: 0,
      init_n: 0,
      evolve_n: 1,
    },
    func: "elm.model_selection.kmeans:kmeans_model_averaging",
  },
  select_top_n: {
    kwargs: {
      top_n: 1,
    },
    func: "elm.model_selection.base:select_top_n_models"
  }
}
ensembles: {
  no_ensemble: {
    init_ensemble_size: 1,  # how many models to initialize at start
    saved_ensemble_size: 1, # how many models to serialize as "best"
    ngen: 1,       # how many model train/select generations
    partial_fit_batches: 1,     # how many partial_fit calls per train/select generation
  },
  small_ensemble: {
    init_ensemble_size: 16,  # how many models to initialize at start
    saved_ensemble_size: 4, # how many models to serialize as "best"
    ngen: 4,       # how many model train/select generations
    partial_fit_batches: 4,     # how many partial_fit calls per train/select generation
  }
}

transform: {
  pca: {
    model_init_class: "sklearn.decomposition:IncrementalPCA",
    model_init_kwargs: {"n_components": 2},
    ensemble: no_ensemble,
    model_scoring: Null,
  }
}

sklearn_preprocessing: {
  min_max: {
    method: minmax_scale,
    feature_range: [0, 1],
    axis: 0,
    copy: False,
  },
  standard: {
    method: StandardScaler,
    copy: False,
    with_mean: True,
    with_std: True,
  },
  poly_2nd_interactions_only: {
    method: PolynomialFeatures,
    degree: 2,
    interaction_only: True,
    include_bias: True,
  },
  poly_2nd: {
    method: PolynomialFeatures,
    degree: 2,
    interaction_only: False,
    include_bias: True,
  },
  log: {
    method: FunctionTransformer,
    func: "numpy:log",
    validate: True,
  },
  log10: {
    method: FunctionTransformer,
    func: "numpy:log10",
    validate: True,
  },
  require_positive: {
    method: FunctionTransformer,
    func: "elm.sample_util.encoding_scaling:require_positive",
    func_kwargs: {small_num: 0.0001},
  },
}

sample_pipelines: {
  log10: [
    {sklearn_preprocessing: require_positive},
    {sklearn_preprocessing: log10},
  ],
  standardize_log10: [
    {sample_pipeline: log10},
    {sklearn_preprocessing: standard},
  ],
  standardize_log10_var_top_80: [
    {sample_pipeline: standardize_log10},
    {feature_selection: top_80_percent},
  ],
  standardize_log10_var_top_80_inter: [
    {sample_pipeline: standardize_log10_var_top_80},
    {sklearn_preprocessing: poly_2nd_interactions_only},
  ],
  standardize_log10_var_top_80_trans: [
    {sample_pipeline: standardize_log10_var_top_80},
    {transform: pca, method: fit_transform},
  ],
  standardize_log10_var_top_80_inter_trans: [
    {sample_pipeline: standardize_log10_var_top_80_inter},
    {transform: pca, method: fit_transform},
  ],
  flatten_example: [
    {select_canvas: band_1},
    {flatten: 'C'},
    {sklearn_preprocessing: require_positive},
    {drop_na_rows: True},
  ],
}
train: {
  kmeans: {
    model_init_class: "sklearn.cluster:MiniBatchKMeans",
    model_init_kwargs: {
      compute_labels: True
    },
    ensemble: no_ensemble,
    output_tag: kmeans,
    model_scoring: kmeans_aic,
    model_selection: kmeans_model_averaging,
  }
}


pipeline:
  - {sample_pipeline: [{sample_pipeline: flatten_example},
                       {transform: pca, method: fit_transform}],
     data_source: NPP_DSRF1KD_L2GD,
     steps: [
       {transform: pca,
        method: partial_fit},
       {train: kmeans,
        method: partial_fit},
       {predict: kmeans},
     ]
  }


# Config options that can be overriden by environment variables:
DASK_THREADS: Null # os.cpu_count() if not given
DASK_PROCESSES: Null  # os.cpu_count() if not given
DASK_EXECUTOR: SERIAL
DASK_SCHEDULER: Null
LADSWEB_LOCAL_CACHE: Null
