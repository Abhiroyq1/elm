One-Year Plan
~~~~~~~~~~~~~
One-Year Plan for Elm and Related Innovation

.. reStructured Text (reST) cheatsheet:

   This indented block is a comment.

   Basics:
      Indentation IS significant in reST, just like in Python.
      Two dots (..) at the beginning of a line denote a "directive", which can be pretty much anything; in this case it's a huge comment.

   Shortcuts:
     *italics (emphasis)*
     **bold (strong)**
     ``code (literal)``
     Other "roles" include: subscript, superscript, title-reference,
         and can be used with the :rolename:`content` syntax.
         More roles here: http://www.sphinx-doc.org/en/stable/markup/inline.html
     "Definition lists" are simply a line followed by a paragraph with hanging indentation.
     "Quoted paragraphs" are just indented definition lists.
     "Line blocks" (starting with pipe chars) preserve line breaks.
     Code blocks are lines ending with ::, followed by an empty line, followed by the code snippet.
     Substitutions: ".. |name| replace:: replacement text"

   Links:
     External links: `Link text <http://example.com/>`_
     Internal links: :ref:`label-name`
         (assuming that there is a ".. _label-name" somewhere in the docs.
     Cross-referencing docs: :doc:`../relative/file/path`
     Download: :download:`example script <../example.py>`
     Footnote: [#]_ somewhere in text, then at end: ".. [#] Text of footnote"

   Gotchas:
       Inline markup cannot have whitespaces in it (e.g. * text* is wrong)
       Inline markup must be separated by non-word characters (e.g. this*is*wrong)
       Inline markup cannot be nested
       Nested lists need to be separated by blank lines


.. _scikit-learn: https://scikit-learn.org
.. _xarray: https://xarray.pydata.org

:ref:`elm` supports workflows of unsupervised or supervised learning models, including clustering algorithms, classification, and regression, including common spatial or statistical preprocessing methods that may be run in a pipeline of transformers with adjustable parameters. :ref:`elm` follows conventions of popular Python machine learning tools, e.g. `scikit-learn`_


.. figure:: NASA_diagram.png
   :alt: Components of Elm

    Figure 1. Components of Elm


Elm’s Capabilities
------------------
* Supervised models

   * elm.pipeline.steps.linear_model

* Unsupervised models

   * Clustering
   * Manifold
   * Decomposition

* Statistical preprocessors and normalization
* Custom estimators
* Multi-model inference
* Hyperparameterization

   * Evolutionary algorithms
      * Custom batch model fitting in generations

* Hierarchical modeling
* Cross validation
* Feature engineering pipelines
* Data structure flexibility
* Parallel machine learning


Use cases
---------

* Data assimilation - Reanalysis
* Elm’s Usage Patterns


Elm can be used in Python, following common usage conventions established tools like `scikit-learn`_ (for machine learning) and `xarray`_ (for spatial data).


* Elm - Usage in Python - The most flexible way to use Elm is conda-installing the Elm package and writing Python scripts that extend Elm model selection classes or instantiate Elm’s Pipeline class.  
* Elm-UI - Elm has a user interface for common machine learning tasks with rasters or multi- or hyperspectral imagery (to be developed later in Phase II).
* Elm-cli - Elm pipelines can be run from a YAML format configuration that is generated by the Elm-UI or created manually.


Elm uses a Pipeline (elm.pipeline.Pipeline) to represent series of transformations on machine learning input data where the transformers may be hyperparameterized (See Pipeline Conventions in the scikit-learn documentation for more reading).  Elm extends the concept of scikit-learn’s Pipeline in the following ways:
* Elm allows most pipeline specifications via a YAML format text file
* Elm Pipeline’s allow transformations on Xarray data structures, e.g. reshaping a collection of rasters or N-D arrays to an input feature matrix, while scikit-learn Pipeline generally works with a 2-D numpy array
* Elm’s Pipeline uses dask for running scikit-learn models in embarrassingly-parallel batches


Elm can be extended for custom workflows by following the scikit-learn patterns for custom estimators


Elm-UI
------

Exploring inputs and results
Feature labeling map tools
Monitoring job progress
Templates for ML pipelines

See also: 
* Proposal: Easy apps and dashboards from notebooks


Elm-cli
-------

Command-line interface
Execute pipeline definition
Override pipeline parameters
Configure dask concurrency


Elm Components
--------------

Xarray 
Scikit-learn
Xarray_filters
Dask
Earthio


Elm-Earthio-NLDAS
-----------------

Elm-Earthio-NLDAS is a repository that experiments with the NLDAS data, e.g. the scripts and notebooks that will create the December AGU poster.  We plan to use this repo in Phase II for assets that are directly related to integration with NLDAS like the AGU poster and future research to operations. The goal is that Elm-Earthio-NLDAS drives ideas and lets us know what is not working in Elm and that general utilities may be extracted from the NLDAS-specific research. For example, the NSGA-2 experiment in the Elm-Earthio-NLDAS repo motivated this issue 185 in Elm to provide a wider range of hyperparameter control options.


PyData / Scipy / Community open source packages
-----------------------------------------------

Deap
Pandas


Background Material on Python Conventions
-----------------------------------------

.. Elm in ideal state:
   * Components:
      * Library 
         * Components:
            * scikit-learn + Dask
               * Milestone 2: Improved Tools for Ensemble Fitting and Prediction
               * Milest[a]one 4: Improved Support for Spectral Clustering / Embedding and Manifold Learning
            * Spatial data - 
               * xarray tools e..g 3-D and 4-D feature engineeering - xarray_filters made big progress
               * Milestone 3: Zonal Statistics, Rolling Statistics, Filters, and Change Detection
                  * Laplacian, gradient filters
         * PR 192 inherit base classes from dask-searchcv (parallel scikit-learn)
         *    * UI
         * UI library - Extension of EarthSim - 780 hours
            * MS-5-Task-1 - Bokeh Map Drawing Tools
               * Use cases: Polygon around a lake and that is the Y data as 1/0 lake mask in a Step called MaskForPoly
            * MS-5-Task-2 - Feature Labeling Tools
               * Use case: See above
            * MS-5-Task-3 - Visualizing Inputs and Predictions
               * Elm’s advantage with xarray basis is transforming predictions from 1-D back to a raster geospatial view
                  * UI take advantage of what was “inverse_transform” in PHase I and is “from_features” (called on a 1-D prediction to go to a 2-D raster e..g a classification integer indicator raster)
         * MS-7-Task-1 - Automated Simple Configurations of Elm - 360 hours
            * Limited to unsupervised
            * Limited to collections of 2-d rasters/bands of satellite images
         * Milestone 7 Task 2: Runtime User Interface for dask.distributed - 360 hours
            * User adds a config (e..g file, environ) that affects dask.distributed UI layout
               * Goal - better be able to monitor the status of ML jobs, e.g. elm-main if possible
            * 1.4 years away?
            * This could involve replanning the specifics with NASA + dask team - still dask.distributed related
         * This is probably less interesting to Dan and Grey
         * How does it relate to Platform:
            * Open source questions (ask Hunt) relative to proposal - what we said regarding open source - UI (I think like everything) has be open
            * Browse documentation offline
      * Yaml config
         * Library you can pass callables that are on-the-fly, e.g. partial - can’t be represented in yaml
         * Refer to your importable BaseEstimator style classes, specifically Step
         * Essentially get_params() output + importable name (func=’numpy.mean’, params=dict(n_cluster=2))
         * BaseEstimator, e.g. elm.pipeline.steps.linear_model.LinearRegression 
            * Spec - >
         * BaseComposition, e.g. elm.pipeline.Pipeline or elm.model_selection.EaSearchCV
            * Spec method refers to the internal .spec methods of subestimators
         * Def spec to/from yaml methods on:
            * Step and/or elm.mldataset.SklearnMixin
            * elm.pipeline.Pipeline
            * elm.model_selection.EaSearchCV
               * Essentially the same as :
                  * dask_searchcv.GridSearchCV
                  * dask_searchcv.RandomizedSearchCV
         * Use Cases:
            * UI dumps one - yes - We have limited for that Automated UI task - and a limited scope of problem (for PHase II).  (Phase II-Extension [Phase III] idea [25K to 375K from some other group])
            * Construct a Pipeline and dump yaml
               * Works for many cases - yaml serialization issues?
                  * E.g. if one of them takes a fitted estimator as an input argument
               * Limit scope where necessary, aiming to support the UI use case best
         * Pipeline.from_yaml_spec(my_filename_string_or_buffer):
            * Gives you Pipeline instance on which you can call 
               * pipe.fit(X, y).predict(X)
         * Param - 
            * Param from yaml or json
            * Jupyter notebook viz with param/yaml
            * Part of Bokeh UI
            * Other use case for Param is:
               * UI reflects data types and parameter choices and validation
            * We need a xarray_filters.Step -> Param method
      * Command line tools
         * Emphasize the Python library approach in the docs, mentioning elm-main if you are using the UI or (later on the data catalog)
         * Elm-main
            * Undeprecate in 2 to 4 months - 
            * runs a Pipeline previously serialized to yaml or typed by hand
               * Elm-main my-pca-kmeans-pipe.yml --scheduler localhost:8786
                  * --pca__n_components 6
                  * --step_1__download_url http:/…..
               * Assumes the Pipeline is inclusive of loading X (and y if needed into a MLDataset - Xarray tools)
               * Pipeline or BaseComposition style class loaded includes a Step with a func=download_something
               * Work needed        
                  * Step class that can take X and y as None in transform and call something like:
                     * Download_data
                  * Xarray_filters - See also the Earthio data catalog
            * Assumes dask scheduler is already running or calls Client() or does not call Client()
            * Usage
               * Provides usage by non-Python users who do the UI
               * Does the UI dump a config? Or does the UI run a specification of some sort?
         * Earthio (not much NASA funding on this data catalog):
            * Data catalog also would involving a yaml spec for download - full build out of download options and namiing of data sets
            * Take terminology for data catalog (service, provider, parameter, etc)
               * https://github.com/ContinuumIO/earthio/issues/16
            *       * Runs one yaml file
         * Single command 
            * Elm-make-example --estimator Poly Kmeans
   * What is dask-ml and how does that relate to elm (post PR 192 and clean up)?
      * Elm -> go away - > subpackages of daskml, like dask-searchcv, dask-glm should do
      * Daskml only has one person working on it 
         * Each go separate paths?
         * Separate approaches - separate problems?
      * Dask team does not want to consume xarray
      * Phase 2 Proposal - and the Condensed version of Phase 2
      * Note we need to do data structure flexibility:
         * MS-6-Task-3 -  Support numpy arrays or pandas data frame input
   * Are we a black box approach or a library?
      * Both - UI has the MS-7-Task-1 - Automated Simple Configurations of Elm
         * Black box
            * Limited to LANDSAT style or Hyperspectral - many rasters or image bands
            * Limited to unsupervised
            * Pipeline.to_yaml_spec -> make templates for:
               * PCA - KMeans
               * UI has drop downs for PCA, Manifold, and the get_params()
               * Or UI guesses - PCA KMeans by default - with __init__ params defaults
               * Or we provide 30 template files for 2 or 3-step Pipeline specs
            * To_yaml_spec
               * Relates to Param - NASA 8 hours of my time went to param serialization
         * Library style:
            * Milestone 6: Data Structure Flexibility
               * Support xarray, numpy, dask
            * Milestone 3: Zonal Statistics, Rolling Statistics, Filters, and Change Detection
            * Milestone 2: Improved Tools for Ensemble Fitting and Prediction
            * Consistency with functionality mentioned in Phase I docs or final report status, e.g. Pipeline has to do most of the functionality shown
               * http://ensemble-learning-models.readthedocs.io/en/latest/pipeline.html
   * Expand on use cases here:
      * http://ensemble-learning-models.readthedocs.io/en/latest/use-cases.html
      * Function links not up to date with PR 192 - move to sklearn style
      * How would I do feature engineering on my data if I work at NASA/USGS/NOAA/USDA:
         * The UI will help (automated configuration UI) - choose and trial and error with PCA, polynomial features, 40 classes or so and their parameters, constructing typically a 2 to 5 step PIpeline and then viz
         * We don’t have to make a black box 
      * Some users want a Pipeline that has nothing to do with ML, e.g. download some data from a URL in a yaml spec and do monthly means and make plot.  Dharhas is a big fan (Earthio - data catalog issue comments)
         * Do we worry about not downloading data again, for example?
            * No - we are not scoped to write a downloader application - that’s the idea for data catalog related SEPARATE funding
               * NASA systems are all NFS-like - they don’t care about this downloading stuff too much internally, but later on would see it as valuable to users of their products
            * What we can do:
               * If I write something that downloads 5 GB to my laptop with my custom function
               * OR it is assumed the data exist or 
               * I use pydap in a Step
      * If someone doesn’t know ML:
         * Use the UI (Automated configs)
            * E.g. Label lakes and use a default 4 step Pipeline to classifiy is-lake 1/0
         *       * Direct to the flow chart - Q&A on your problem - are you doing regression do you X and Y downloaded.
         * Refer to scikit-learn docs, dask, etc
      * New user of Elm:
         * Write Python
         * I write a yaml and use elm-main
         * Use the UI
      * Installation plans:
         * elm by itself
            * Just ML no Grib or NetCDF support
         * Elm + earthio
            * Get all ML + file type tools + datashader/holoviews + Jupyter
         * Elm + xarray_filters
            * Get ML + many file tools, e.g. NetCDF but not Grib and not rasterio 
         * Xarray_filters alone
            * Xarray without all the addons like Grib
         * Earthio alone
            * All file support + xarray_filters (not sure on Elm)
            * See https://github.com/ContinuumIO/earthio/issues/31
               * Does load_array need a big refactor - e.g. move to xarray repo or xarray conventions
            * Currently not much in Earthio other than:
               * Installs all data file tools, Grib, NetCDF
               * Load_array -> GeoTiffs in elm/examples - hardening task at the end unless we see an xarray issue we comment on
               * Near term is keep earthio with data loaders and the environment with everything
         * daskml 
         *  run the notebooks inclusive of 
            * * Recent work:
      * MS-6-Task-2 - Feature engineering options for 3-D and 4-D data
   
